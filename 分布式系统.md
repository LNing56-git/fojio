> 抱佛脚一时爽，一直抱佛脚一直爽！这篇文章总结常见的分布式系统面试问题~因为是抱佛脚，所以结构上没有什么逻辑...
>
> 参考链接：[Waking-Up](https://github.com/wolverinn/Waking-Up)  [CycNotes](https://github.com/CyC2018/CS-Notes)  [牛客网](https://www.nowcoder.com/) 



# 分布式锁

### zookeeper实现分布式锁

- 创建一个锁目录 /lock；
- 当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；
- 客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；
- 执行业务代码，完成后，删除对应的子节点
- 羊群效应：一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应，一只羊动起来，其它羊也会一哄而上），而我们只希望它的后一个子节点收到通知

### redis实现分布式锁

- setnx
  - 根据lock的Key区进行setnx（set not exist，如果key不存在，则正常设置，返回1，否则不会进行设置并返回0）操作，如果设置成功，表示已经获得锁，否则并没有获取锁
  - 如果没有获得锁，去Redis上拿到该key对应的kv对的过期时间；如果该key未过期，返回false，表示其他人正在占用该key，不能强制使用；如果已经过期，那我们就可以进行解锁，并更新过期时间
  - 如果在setnx失败后，get该key却无法拿到该字段时，说明操作之前该锁已经被释放，这个时候，最好的办法就是重新执行一遍setnx方法来获取其值以获得该锁
- watch
  - 类似于CAS
  - 重试n次，n次返回均未成功，则接口返回失败

# 分布式事务

事务的操作位于不同的节点上，需要保证事务的 ACID 特性；实现方法：

### 2PC：两阶段提交

- 过程
  - 协调者询问多个参与者事务是否执行成功，参与者发回事务执行结果
  - 如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务
- 缺点
  - 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞等待状态，无法进行其它操作
  - 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响
  - 在提交阶段，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致
  - 任意一个节点失败就会导致整个事务失败，没有完善的容错机制

### 本地消息表

- 本地消息表与业务数据表处于同一个数据库中
- 过程
  - 完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中
  - 将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发
  - 分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作

# CAP特性：三选二

### 一致性 C

- 多个数据副本保持一致
- 强一致性：对系统的一个数据更新成功之后，所有用户都能够读取到最新的值

### 可用性 A

指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的

### 分区容忍性 P

分布式系统在遇到任何网络分区（节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信）故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障

### 权衡

在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际上是要在可用性和一致性之间做权衡

- 为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性
- 为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致
- BASE策略
  - 基本可用（BA）：分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性
  - 软状态（S）：允许一致性状态之间有中间状态，即允许系统不同节点的数据副本之间进行同步的过程存在时延
  - 最终一致性（E）：系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态

# paxos

### 作用

用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值

### 流程

- 第一阶段：Prepare阶段。Proposer向Acceptors发出Prepare请求，Acceptors针对收到的Prepare请求进行Promise承诺
- 第二阶段：Accept阶段。Proposer收到多数Acceptors承诺的Promise后，向Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理
- 第三阶段：Learn阶段。Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners

# raft

### 作用

竞选主节点

### 竞选流程

- 有三种节点：Follower、Candidate 和 Leader
- Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段
- 变成candidate的节点发送投票请求给其它所有节点，其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader；如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票

### 数据同步过程

- 来自客户端的修改都会被传入 Leader。注意leader的该修改还未被提交，只是写入日志中
- Leader 会把修改复制到所有 Follower
- Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交
- Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致

# zab

zookeeper使用的策略；在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞

### 消息广播过程

- leader接收到消息请求后，将消息赋予一个全局唯一的zxid
- leader将带有zxid的消息作为一个提案（proposal）分发给所有的 follower
- follower接收到proposal，先把proposal写到磁盘，写入成功以后再向leader回复一个ack
- leader接收到合法数量（超过半数节点）的ack后，leader就会向这些follower发送commit命令，同时会在本地commit
- 当follower收到消息的commit命令以后，会提交该消息

### 崩溃恢复过程

- 新选举Leader：选举拥有集群中所有机器最高编号（ZXID 最大）的事务Proposal的机器为新leader，从而保证这个新选举出来的 Leader 一定具有所有已经提交的提案
- Leader 服务器确认事务是否都已经被过半的 Follwer 提交了，即是否完成了数据同步
- 等到某个Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表并开始之后的其他流程

# 负载均衡

### 负载均衡算法

- 轮询：把每个请求轮流发送到每个服务器上
- 加权轮询：根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值
- 最少连接：将请求发送给当前最少连接数的服务器上；由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡
- 加权最少连接：在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数
- 随机算法
- 源地址哈希法：对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号；可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞（Sticky Session）

### 转发方法

- http重定向

  使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，状态码为 302。客户端收到重定向报文之后，需要重新向服务器发起请求

- DNS 域名解析

  在 DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址

- 反向代理服务器

  用户的请求需要先经过反向代理服务器才能到达源服务器

- 让负载均衡服务器同时作为集群的网关服务器

  在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发

- 直接路由

  在链路层根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发

  通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈

# 集群session管理

一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作

### sticky session

配置负载均衡器，使得一个用户的所有请求都路由到同一个服务器，这样就可以把用户的 Session 存放在该服务器中

### Session Replication

在服务器之间进行 Session 同步操作，每个服务器都有所有用户的 Session 信息

### Session Server

使用一个单独的服务器存储 Session 数据

# 缓存问题

### 缓存穿透

- 对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库
- 解决方法
  - 对这些不存在的数据缓存一个空数据
  - 对这类请求进行过滤

### 缓存雪崩

- 由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库
- 解决方法
  - 通过观察用户行为，合理设置缓存过期时间
  - 使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用
  - 进行缓存预热（提前将热点数据加载到缓存中），避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩

### 缓存一致性

- 要求数据更新的同时缓存数据也能够实时更新
- 解决方法
  - 在数据更新的同时立即去更新缓存
  - 在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新

# 一致性哈希

### 作用

克服传统哈希分布在服务器节点数量变化时大量数据迁移的问题

### 原理

将哈希空间 [0, 2^n-1]（这里的n与节点数量无关） 看成一个哈希环，每个服务器节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上

### 数据倾斜问题

- 服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）
- 解决方法：虚拟节点机制
  - 每个机器节点会进行多次哈希，最终每个机器节点在哈希环上会有多个虚拟节点存在
  - 数据定位算法不变，只是多了一步虚拟节点到实际节点的映射

# 分布式 id 生成算法

### snowflake

- 你的某个服务假设要生成一个全局唯一 id，那么就可以发送一个请求给部署了 SnowFlake 算法的系统，由这个 SnowFlake 算法系统来生成唯一 id
- 一共64bit：1bit-无用（因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0） + 41bit-时间戳（单位是毫秒） + 10bit-工作机器Id（所以这个服务最多可以部署在 2^10 台机器上） + 12bit序列号（即某台机器上这一毫秒内同时生成的 id 的序号）
- 优点
    - 高性能高可用：生成时不依赖于数据库，完全在内存中生成
    - 容量大：每秒中能生成数百万的自增ID
    - ID自增：之后存入数据库中时索引效率高
- 缺点：依赖与系统时间的一致性，如果系统时间被回调，或者改变，可能会造成id冲突或者重复
- 优化：机器没有那么多时，可以不需要用10bit来做工作机器id
- 应用场景：论坛的例子，论坛底下是一层一层的，每一条留言对应一个ID，有序且唯一

### 基于redis的原子操作INCR和INCRBY

假如一个集群中有5台Redis，可以初始化每台Redis的值分别是1,2,3,4,5，步长都是5，各Redis生成的ID如下：
A：1,6,11,16
B：2,7,12,17
C：3,8,13,18
D：4,9,14,19
E：5,10,15,20

# 时间同步协议

### NTP

- 流程
  - 在Device A和Device B的系统时钟同步之前，Device A的时钟设定为10:00:00am，Device B的时钟设定为11:00:00am
  - Device A发送一个NTP报文给Device B，该报文带有它离开Device A时的时间戳，该时间戳为10:00:00am（T1）
  - 当此NTP报文到达Device B时，Device B加上自己的时间戳，该时间戳为11:00:01am（T2）
  - 当此NTP报文离开Device B时，Device B再加上自己的时间戳，该时间戳为11:00:02am（T3）
  - 当Device A接收到该响应报文时，Device A的本地时间为10:00:03am（T4）
  - A计算得到：Delay=（T4-T1）-（T3-T2）=2秒；offset=（（T2-T1）+（T3-T4））/2=1小时，从而可以设置自己的时钟完成同步了

- 误差
    - 打时间戳的位置在应用层，受协议栈缓存、任务调度等影响，不能保证一到达就打时间戳

### PTP

与NTP类似，但PTP是亚微秒级的，NTP是毫秒级的